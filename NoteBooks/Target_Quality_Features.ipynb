{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import os.path\n",
    "from statistics import median\n",
    "import sys\n",
    "from os import path\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_nodes_features(dataset, policy):\n",
    "    # To get distance ##################\n",
    "    f = open(f\"{dataset}/out_graph.txt\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    G = nx.parse_edgelist(lines[1:], nodetype=int, create_using=nx.DiGraph)\n",
    "    trgs = dict()\n",
    "    min_d = dict()\n",
    "    sum_d = dict()\n",
    "    max_d = dict()\n",
    "    no_exist = dict()\n",
    "    median_d = dict()\n",
    "    ####################################\n",
    "    \n",
    "    round_stats = pd.read_csv(f\"{dataset}/sc_{policy}.csv\").drop(axis=0, labels=[0])\n",
    "    target_nodes = dict()\n",
    "    for index, row in round_stats.iterrows():\n",
    "        edges = eval(row[\"Edges\"])\n",
    "        # ---------------------------------------------------\n",
    "        for edge in edges:\n",
    "            target = edge[1]\n",
    "            if target not in target_nodes:\n",
    "                target_nodes[target] = [0 for i in range(10)]\n",
    "            target_nodes[target][row[\"Rounds\"]] += 1\n",
    "        # ---------------------------------------------------\n",
    "        # To get distance #####################\n",
    "        for e in edges:\n",
    "            trgs.setdefault(e[1], 0)\n",
    "            min_d.setdefault(e[1], sys.maxsize)\n",
    "            sum_d.setdefault(e[1], 0)\n",
    "            max_d.setdefault(e[1], 0)\n",
    "            no_exist.setdefault(e[1], 0)\n",
    "            median_d.setdefault(e[1], list())\n",
    "            trgs[e[1]] += 1\n",
    "            try:\n",
    "                length = len(nx.shortest_path(G, source=e[1], target=e[0])) - 1\n",
    "                sum_d[e[1]] += length\n",
    "                if min_d[e[1]] > length: min_d[e[1]] = length\n",
    "                if max_d[e[1]] < length: max_d[e[1]] = length\n",
    "                median_d[e[1]].append(length)\n",
    "            except nx.NetworkXNoPath:\n",
    "                no_exist[e[1]] += 1\n",
    "        #######################################\n",
    "    totals = dict()\n",
    "    for node in target_nodes:\n",
    "        totals[node] = sum(target_nodes[node])\n",
    "    nodes = list(totals.keys())\n",
    "    values = list(totals.values())\n",
    "    dd = {\n",
    "        \"Nodes\": nodes,\n",
    "        \"times\": values\n",
    "    }\n",
    "    df = pd.DataFrame(dd)\n",
    "    nodes_info = pd.read_csv(f\"{dataset}/nodeQualityFeatures.txt\", sep=\"\\t\")\n",
    "    nodes_info['out_homophily'] = [row['redNeighborsOutRatio'] if row['group'] == 1 else 1 - row['redNeighborsOutRatio']\n",
    "                               for i, row in nodes_info.iterrows()]\n",
    "    df = df.join(nodes_info.set_index(\"nodeId\"), on=\"Nodes\")\n",
    "    \n",
    "    # To get distance ########################\n",
    "    data = list()\n",
    "    for k,v in trgs.items():\n",
    "        temp_median = median(median_d[k]) if median_d[k] else float(\"NaN\")\n",
    "        data.append([k, min_d[k], max_d[k], round(sum_d[k]/trgs[k],3), temp_median, no_exist[k]])\n",
    "    df_d = pd.DataFrame(data, columns=['Nodes', 'MinDistance', \"MaxDistance\", \"AverageDistance\", \"MedianDistance\", \"NoExistsDistance\"])\n",
    "    df = df.join(df_d.set_index(\"Nodes\"), on=\"Nodes\")\n",
    "    ##########################################\n",
    "    \n",
    "    df = df.sort_values(axis=0, by=\"times\", ascending=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_n2v = get_target_nodes_features(\"books\", \"node2vec\")\n",
    "blogs_n2v = get_target_nodes_features(\"blogs\", \"node2vec\")\n",
    "dblp_n2v = get_target_nodes_features(\"dblp_course\", \"node2vec\")\n",
    "dblp_n2v_n2v = get_target_nodes_features(\"dblp_course\", \"node2vec\")\n",
    "twitter_n2v = get_target_nodes_features(\"twitter\", \"node2vec\")\n",
    "\n",
    "books_fair = get_target_nodes_features(\"books\", \"fair\")\n",
    "blogs_fair = get_target_nodes_features(\"blogs\", \"fair\")\n",
    "dblp_fair = get_target_nodes_features(\"dblp_course\", \"fair\")\n",
    "dblp_fair_fair = get_target_nodes_features(\"dblp_course\", \"fair\")\n",
    "twitter_fair = get_target_nodes_features(\"twitter\", \"fair\")\n",
    "\n",
    "books_h = get_target_nodes_features(\"books\", \"hybrid_node2vec\")\n",
    "blogs_h = get_target_nodes_features(\"blogs\", \"hybrid_node2vec\")\n",
    "dblp_h = get_target_nodes_features(\"dblp_course\", \"hybrid_node2vec\")\n",
    "dblp_h_h = get_target_nodes_features(\"dblp_course\", \"hybrid_node2vec\")\n",
    "twitter_h = get_target_nodes_features(\"twitter\", \"hybrid_node2vec\")\n",
    "\n",
    "books_b = get_target_nodes_features(\"books\", \"dyadic_fair\")\n",
    "blogs_b = get_target_nodes_features(\"blogs\", \"dyadic_fair\")\n",
    "dblp_b = get_target_nodes_features(\"dblp_course\", \"dyadic_fair\")\n",
    "dblp_b= get_target_nodes_features(\"dblp_course\", \"dyadic_fair\")\n",
    "twitter_b = get_target_nodes_features(\"twitter\", \"dyadic_fair\")\n",
    "\n",
    "books_hb = get_target_nodes_features(\"books\", \"hybrid_balanced_node2vec\")\n",
    "blogs_hb = get_target_nodes_features(\"blogs\", \"hybrid_balanced_node2vec\")\n",
    "dblp_hb = get_target_nodes_features(\"dblp_course\", \"hybrid_balanced_node2vec\")\n",
    "dblp_hb = get_target_nodes_features(\"dblp_course\", \"hybrid_balanced_node2vec\")\n",
    "twitter_hb = get_target_nodes_features(\"twitter\", \"hybrid_balanced_node2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\hline\n",
      "Node2vec & 2.989857 & 3.071429 & 3.714286 & 0.000000 & 0.019558 & 0.018066 & 0.028284 & 0.352294 & 0.030159 & 0.966886 & 0.919048 & 1.000000 & 1.000000 & 0.279070 \\\\\n",
      "\\hline\n",
      "Fair & 2.568500 & 2.400000 & 3.300000 & 0.000000 & 0.007763 & 0.007787 & 0.010310 & 0.969345 & 0.969804 & 0.973552 & 1.000000 & 1.000000 & 1.000000 & 1.000000 \\\\\n"
     ]
    }
   ],
   "source": [
    "# Books.\n",
    "books_top = books_n2v.loc[books_n2v['times'] >= 6]\n",
    "temp = books_top.describe().iloc[[1, 2, 3, 5, 7]]\n",
    "print(\"\\\\hline\")\n",
    "print(f\"Node2vec\\\n",
    " & {temp.AverageDistance['mean']:.6f} & {temp.MedianDistance['mean']:.6f} & {temp.MaxDistance['mean']:.6f} & {temp.NoExistsDistance['mean']:.6f}\\\n",
    " & {temp['pagerank']['mean']:.6f} & {temp['pagerank']['50%']:.6f} & {temp['pagerank']['max']:.6f}\\\n",
    " & {temp['redPagerank']['mean']:.6f} & {temp['redPagerank']['50%']:.6f} & {temp['redPagerank']['max']:.6f}\\\n",
    " & {temp['out_homophily']['mean']:.6f} & {temp['out_homophily']['50%']:.6f} & {temp['out_homophily']['max']:.6f}\\\n",
    " & {sum((books_top.times * books_top.group) / sum(books_top.times)):.6f} \\\\\\\\\")\n",
    "\n",
    "books_top = books_fair.loc[books_fair['times'] >= 5]\n",
    "temp = books_top.describe().iloc[[1, 2, 3, 5, 7]]\n",
    "print(\"\\\\hline\")\n",
    "print(f\"Fair\\\n",
    " & {temp.AverageDistance['mean']:.6f} & {temp.MedianDistance['mean']:.6f} & {temp.MaxDistance['mean']:.6f} & {temp.NoExistsDistance['mean']:.6f}\\\n",
    " & {temp['pagerank']['mean']:.6f} & {temp['pagerank']['50%']:.6f} & {temp['pagerank']['max']:.6f}\\\n",
    " & {temp['redPagerank']['mean']:.6f} & {temp['redPagerank']['50%']:.6f} & {temp['redPagerank']['max']:.6f}\\\n",
    " & {temp['out_homophily']['mean']:.6f} & {temp['out_homophily']['50%']:.6f} & {temp['out_homophily']['max']:.6f}\\\n",
    " & {sum((books_top.times * books_top.group) / sum(books_top.times)):.6f} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
